# IncidentFox Local Development Configuration
#
# This file is used in local development mode (CONFIG_MODE=local).
# It provides a declarative way to configure your local IncidentFox instance.
#
# Secrets: Use ${ENV_VAR} syntax to reference environment variables from .env
# Example: api_key: ${CORALOGIX_API_KEY}
#
# When you configure integrations via Slack or the Web UI, this file will be
# automatically updated to reflect your changes.

# Organization and Team
org_id: local
team_id: default

# AI Model Configuration
# Set provider + model_id to choose which AI powers agent runs.
# The corresponding API key must be set in the integrations section below.
#
# Providers and recommended models (as of Feb 2026):
#
#   provider: anthropic    → integrations.anthropic.api_key
#     model_id: claude-sonnet-4-6         # latest flagship (default)
#     model_id: claude-opus-4-6-20260205  # most capable
#     model_id: claude-opus-4-5-20251101  # previous opus generation
#     model_id: claude-sonnet-4-5-20250929
#     model_id: claude-haiku-4-5-20251001 # fast & cheap
#
#   provider: openai       → integrations.openai.api_key
#     model_id: gpt-4o                    # flagship
#     model_id: gpt-4o-mini               # fast & cheap
#     model_id: gpt-4.1                   # latest GPT-4 series
#     model_id: o3                        # advanced reasoning
#     model_id: o4-mini                   # fast reasoning
#
#   provider: gemini       → integrations.gemini.api_key
#     model_id: gemini-2.5-pro            # most capable
#     model_id: gemini-2.5-flash          # fast & affordable
#     model_id: gemini-2.0-flash          # stable fast model
#     model_id: gemini-1.5-pro            # solid, widely supported
#
#   provider: deepseek     → integrations.deepseek.api_key
#     model_id: deepseek-chat             # general purpose (DeepSeek V3)
#     model_id: deepseek-reasoner         # reasoning (DeepSeek R1)
#     model_id: deepseek-v3               # explicit V3
#
#   provider: xai          → integrations.xai.api_key
#     model_id: grok-3                    # latest flagship
#     model_id: grok-3-beta               # stable grok-3
#     model_id: grok-2-latest             # previous gen, stable
#
#   provider: groq         → integrations.groq.api_key  (fast inference)
#     model_id: llama-3.3-70b-versatile
#     model_id: meta-llama/llama-4-scout-17b-16e-instruct
#     model_id: meta-llama/llama-4-maverick-17b-128e-instruct
#
#   provider: mistral      → integrations.mistral.api_key
#     model_id: mistral-large-latest      # flagship
#     model_id: mistral-medium-latest     # balanced
#     model_id: mistral-small-latest      # fast & cheap
#     model_id: codestral-latest          # code-specialized
#
#   provider: cohere       → integrations.cohere.api_key
#     model_id: command-a-03-2025         # latest flagship
#     model_id: command-r-plus            # balanced
#     model_id: command-r-08-2024
#
#   provider: ollama       → no API key needed (runs locally)
#     model_id: llama3.2                  # any model you've pulled
#     model_id: mistral                   # see https://ollama.com/library
#     base_url: http://host.docker.internal:11434
#
#   provider: openrouter   → integrations.openrouter.api_key
#     model_id: anthropic/claude-sonnet-4-6  # route any model via OpenRouter
#     # Full list: https://openrouter.ai/models
#
ai_model:
  provider: anthropic  # see options above
  model_id: claude-sonnet-4-6
  # Optional: Override base URL (required for ollama, azure, bedrock)
  # base_url: http://host.docker.internal:11434

# Integrations
# Configure your observability, cloud, and collaboration tools.
# Only uncomment the sections you need — unused integrations are ignored.
integrations:
  # Coralogix - Log management and observability
  coralogix:
    api_key: ${CORALOGIX_API_KEY}
    region: us2  # us1, us2, eu1, eu2, ap1, ap2
    application_name: incidentfox-local
    subsystem_name: sre-agent

  # Anthropic - API key for Anthropic models (claude-*)
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}

  # OpenAI - API key for OpenAI models (gpt-*, o1, o3, o4-*)
  openai:
    api_key: ${OPENAI_API_KEY}

  # Google Gemini - API key for Gemini models
  # gemini:
  #   api_key: ${GEMINI_API_KEY}

  # xAI (Grok) - API key for grok-* models
  # xai:
  #   api_key: ${XAI_API_KEY}

  # DeepSeek - API key for deepseek-* models
  # deepseek:
  #   api_key: ${DEEPSEEK_API_KEY}

  # Groq - API key for fast Groq-hosted inference
  # groq:
  #   api_key: ${GROQ_API_KEY}

  # Mistral AI - API key for mistral-* and codestral-* models
  # mistral:
  #   api_key: ${MISTRAL_API_KEY}

  # Cohere - API key for command-* models
  # cohere:
  #   api_key: ${COHERE_API_KEY}

  # OpenRouter - single API key to access any provider
  # openrouter:
  #   api_key: ${OPENROUTER_API_KEY}

  # AWS - Cloud infrastructure
  # aws:
  #   access_key_id: ${AWS_ACCESS_KEY_ID}
  #   secret_access_key: ${AWS_SECRET_ACCESS_KEY}
  #   region: us-west-2
  #   # Optional: Specific services
  #   # eks_cluster_name: my-cluster
  #   # rds_instance_id: my-database

  # GitHub - Code repository and CI/CD
  # github:
  #   token: ${GITHUB_TOKEN}
  #   # Optional: Specific repos to watch
  #   # repositories:
  #   #   - owner/repo1
  #   #   - owner/repo2

  # Prometheus - Metrics and alerting
  # prometheus:
  #   url: ${PROMETHEUS_URL}
  #   # Optional: Basic auth
  #   # username: ${PROMETHEUS_USERNAME}
  #   # password: ${PROMETHEUS_PASSWORD}

  # Grafana - Dashboards and visualization
  # grafana:
  #   url: ${GRAFANA_URL}
  #   api_key: ${GRAFANA_API_KEY}

  # PagerDuty - Incident management
  # pagerduty:
  #   api_key: ${PAGERDUTY_API_KEY}
  #   # Optional: Specific service IDs to monitor
  #   # service_ids:
  #   #   - PXXXXXX

  # Datadog - APM and monitoring
  # datadog:
  #   api_key: ${DATADOG_API_KEY}
  #   app_key: ${DATADOG_APP_KEY}
  #   site: datadoghq.com  # or datadoghq.eu, us3.datadoghq.com, etc.

  # Sentry - Error tracking
  # sentry:
  #   auth_token: ${SENTRY_AUTH_TOKEN}
  #   organization: my-org
  #   # Optional: Specific projects
  #   # projects:
  #   #   - my-project

  # Kubernetes - Container orchestration
  # kubernetes:
  #   config_path: ${KUBECONFIG}
  #   # Or use in-cluster config:
  #   # use_in_cluster_config: true
  #   # context: my-context  # Optional: Specific context

# Prompts and Templates
# Customize how the AI agent behaves and responds
prompts:
  # System prompt for the SRE agent
  # Uncomment to customize:
  # system_prompt: |
  #   You are IncidentFox, an expert SRE assistant...

  # Incident investigation prompt
  # Uncomment to customize:
  # investigation_prompt: |
  #   When investigating an incident, follow these steps:
  #   1. Gather context
  #   2. Check recent deployments
  #   3. Analyze logs and metrics
  #   4. Identify root cause
  #   5. Suggest remediation

# Skills and Tools
# Enable or disable specific capabilities
skills:
  # Enable all skills by default
  # To disable specific skills, set to false:
  # - coralogix: true
  # - aws: true
  # - github: true
  # - kubernetes: true

# Routing (auto-managed — do not edit)
# In local mode the slack-bot auto-discovers the workspace ID via auth.test()
# at startup and registers it here.  You only need to set this manually if you
# have multiple Slack workspaces and want explicit control over routing.
# routing:
#   slack_workspace_ids:
#     - T09UF9JAHD1  # your Slack workspace ID (find it in slack-bot logs)

# Security Policies (optional)
# Configure guardrails and permissions
security:
  # Allowed actions - restrict what the agent can do
  # allowed_actions:
  #   - read_only  # Only read operations, no writes/deletes
  #   - deploy     # Can trigger deployments
  #   - scale      # Can scale resources

  # Require approval for sensitive operations
  # require_approval_for:
  #   - kubernetes.delete
  #   - aws.terminate_instance
  #   - github.merge_pr
