"""Credential resolver ext_authz service.

Injects credentials for outgoing requests based on JWT-authenticated sandbox identity.
Supports multiple credential sources:
- environment: Load from env vars (local dev, self-hosted)
- config_service: Fetch from Config Service (SaaS)

Security: Sandboxes are UNTRUSTED (could execute malicious code via prompt injection).
JWT validation ensures only legitimate sandboxes get credentials:
1. Server generates JWT with tenant/team when creating sandbox
2. JWT is embedded in per-sandbox Envoy config
3. Envoy adds x-sandbox-jwt header to ext_authz requests
4. We validate JWT and extract tenant/team (ignoring spoofed headers)
"""

import logging
import os
from contextlib import asynccontextmanager

from cachetools import TTLCache
from fastapi import FastAPI, HTTPException, Request, Response
from pydantic import BaseModel

from .config_client import ConfigServiceClient
from .domain_mapping import get_integration_for_host
from .jwt_auth import validate_sandbox_jwt

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Credential source: "config_service" (SaaS) or "environment" (local/self-hosted)
CREDENTIAL_SOURCE = os.getenv("CREDENTIAL_SOURCE", "environment")

# JWT validation mode: "strict" (require valid JWT) or "permissive" (allow missing JWT for local dev)
JWT_MODE = os.getenv("JWT_MODE", "strict")

# Cache for credentials (5-minute TTL)
credential_cache: TTLCache = TTLCache(maxsize=1000, ttl=300)

# Config Service client (only initialized if needed)
config_client: ConfigServiceClient | None = None

# Environment-based credentials (for local/self-hosted mode)
# Loaded at startup from environment variables
ENV_CREDENTIALS: dict[str, dict] = {}


def load_env_credentials() -> dict[str, dict]:
    """Load credentials from environment variables."""
    return {
        "anthropic": {
            "api_key": os.getenv("ANTHROPIC_API_KEY"),
        },
        "coralogix": {
            "api_key": os.getenv("CORALOGIX_API_KEY"),
            "domain": os.getenv("CORALOGIX_DOMAIN"),
            "region": os.getenv("CORALOGIX_REGION"),
        },
        "confluence": {
            "url": os.getenv("CONFLUENCE_URL"),
            "email": os.getenv("CONFLUENCE_EMAIL"),
            "api_token": os.getenv("CONFLUENCE_API_TOKEN"),
        },
    }


def mask_secret(value: str | None, visible_chars: int = 6) -> str:
    """Mask a secret, showing only first few characters."""
    if not value:
        return "(not set)"
    if len(value) <= visible_chars:
        return "*" * len(value)
    return value[:visible_chars] + "..." + "*" * 8


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    global config_client, ENV_CREDENTIALS

    logger.info(
        f"Starting credential-resolver with source={CREDENTIAL_SOURCE}, jwt_mode={JWT_MODE}"
    )

    if CREDENTIAL_SOURCE == "config_service":
        config_client = ConfigServiceClient()
        logger.info("Config Service client initialized")
    else:
        ENV_CREDENTIALS = load_env_credentials()
        # Check different primary keys based on integration type
        configured = []
        for k, v in ENV_CREDENTIALS.items():
            if k == "confluence":
                if v.get("url") and v.get("api_token"):
                    configured.append(k)
            elif v.get("api_key"):
                configured.append(k)
        logger.info(f"Environment credentials loaded for: {configured}")

        # Debug: show masked credentials to verify they're loaded
        for integration, creds in ENV_CREDENTIALS.items():
            if integration == "confluence":
                url = creds.get("url")
                api_token = creds.get("api_token")
                logger.info(
                    f"  {integration}: url={url or '(not set)'}, "
                    f"api_token={mask_secret(api_token)}"
                )
            else:
                api_key = creds.get("api_key")
                logger.info(f"  {integration}: api_key={mask_secret(api_key)}")

    yield

    if config_client:
        await config_client.close()


app = FastAPI(
    title="Credential Resolver",
    description="ext_authz service for credential injection with JWT authentication",
    version="0.2.0",
    lifespan=lifespan,
)


class ExtAuthzResponse(BaseModel):
    """Response model for ext_authz check."""

    status: str = "ok"
    headers: dict[str, str] = {}


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "source": CREDENTIAL_SOURCE, "jwt_mode": JWT_MODE}


@app.get("/api/integrations")
async def list_integrations(request: Request):
    """List available integrations for this tenant/team.

    Returns non-sensitive metadata about which integrations are configured.
    This can be used in system prompts to inform the agent what's available.

    Security: Tenant/team context is extracted from the validated JWT.
    """
    logger.info("Integration list request")

    # Validate JWT and extract tenant context
    tenant_id, team_id, sandbox_name = await extract_tenant_context(request)

    logger.info(
        f"Integration list: tenant={tenant_id}, team={team_id}, sandbox={sandbox_name}"
    )

    # Check which integrations are configured
    # All active integrations from onboarding.py (excludes anthropic - that's the LLM provider)
    ACTIVE_INTEGRATIONS = [
        "coralogix",
        "incident_io",
        "confluence",
        "grafana",
        "elasticsearch",
        "datadog",
        "prometheus",
        "jaeger",
        "kubernetes",
        "github",
    ]

    available = []
    for integration_id in ACTIVE_INTEGRATIONS:
        creds = await get_credentials(tenant_id, team_id, integration_id)
        if is_integration_configured(integration_id, creds):
            # Return non-sensitive metadata only
            metadata = get_integration_metadata(integration_id, creds)
            available.append({"id": integration_id, **metadata})

    return {"integrations": available}


def is_integration_configured(integration_id: str, creds: dict | None) -> bool:
    """Check if an integration has valid credentials configured.

    Each integration has different required fields based on its authentication method.
    See slack-bot/onboarding.py for field definitions.
    """
    if not creds:
        return False

    # Integrations that require domain + api_key
    if integration_id in ["confluence", "grafana", "kubernetes"]:
        return bool(creds.get("domain") and creds.get("api_key"))

    # Datadog requires api_key + app_key + site (domain)
    if integration_id == "datadog":
        return bool(creds.get("api_key") and creds.get("app_key") and creds.get("site"))

    # Elasticsearch: domain required, credentials optional (some clusters are open)
    if integration_id == "elasticsearch":
        return bool(creds.get("domain"))

    # Prometheus/Jaeger: only domain required (auth optional)
    if integration_id in ["prometheus", "jaeger"]:
        return bool(creds.get("domain"))

    # GitHub: api_key required (domain optional for GHE)
    if integration_id == "github":
        return bool(creds.get("api_key"))

    # Default: api_key required (coralogix, incident_io, etc.)
    return bool(creds.get("api_key"))


def get_integration_metadata(integration_id: str, creds: dict) -> dict:
    """Get non-sensitive metadata about an integration.

    SECURITY: Never return secrets (api_key, api_token, password, etc).
    Only return configuration metadata that helps the agent use the integration.
    """
    if integration_id == "confluence":
        # Return domain URL so agent knows which Confluence instance
        return {"url": creds.get("domain")}

    elif integration_id == "coralogix":
        # Return domain/region for API endpoint construction
        return {
            "domain": creds.get("domain"),
            "region": creds.get("region"),
        }

    elif integration_id == "datadog":
        # Return site for API endpoint construction
        return {"site": creds.get("site")}

    elif integration_id == "grafana":
        # Return URL so agent knows which Grafana instance
        return {"url": creds.get("domain")}

    elif integration_id == "elasticsearch":
        # Return URL and optional default index pattern
        metadata = {"url": creds.get("domain")}
        if creds.get("index_pattern"):
            metadata["index_pattern"] = creds.get("index_pattern")
        return metadata

    elif integration_id == "prometheus":
        # Return URL
        return {"url": creds.get("domain")}

    elif integration_id == "jaeger":
        # Return URL
        return {"url": creds.get("domain")}

    elif integration_id == "kubernetes":
        # Return URL and optional default namespace
        metadata = {"url": creds.get("domain")}
        if creds.get("namespace"):
            metadata["namespace"] = creds.get("namespace")
        return metadata

    elif integration_id == "github":
        # Return enterprise URL if configured (None means github.com)
        metadata = {}
        if creds.get("domain"):
            metadata["url"] = creds.get("domain")
        if creds.get("default_org"):
            metadata["default_org"] = creds.get("default_org")
        return metadata

    # Default: just indicate it's configured (incident_io, etc.)
    return {}


# IMPORTANT: Route ordering matters in FastAPI/Starlette!
# More specific routes MUST be declared BEFORE catch-all routes.
# /confluence/{path:path} must come before /{path:path}


async def generic_proxy(
    integration_id: str,
    path: str,
    request: Request,
    require_api_key: bool = True,
    ssl_verify: bool = True,
) -> Response:
    """Generic reverse proxy for integrations with customer-specific URLs.

    Since customer URLs are specific (e.g., grafana.company.com), we can't use
    Envoy's static routing. This proxy:
    1. Validates JWT and extracts tenant context
    2. Looks up the customer's URL and credentials
    3. Forwards the request with auth headers
    4. Returns the response

    Security: Credentials never leave this service.

    Args:
        integration_id: Integration name (e.g., "grafana", "elasticsearch")
        path: Request path to forward
        request: Original FastAPI request
        require_api_key: Whether to require api_key (False for optional auth)
        ssl_verify: Whether to verify SSL certificates
    """
    import re

    import httpx

    logger.info(f"{integration_id.title()} proxy: {request.method} /{path}")

    # Validate JWT and extract tenant context
    tenant_id, team_id, sandbox_name = await extract_tenant_context(request)

    logger.info(
        f"{integration_id.title()} proxy: tenant={tenant_id}, team={team_id}, sandbox={sandbox_name}"
    )

    # Get credentials
    creds = await get_credentials(tenant_id, team_id, integration_id)
    if not creds or not creds.get("domain"):
        logger.error(f"{integration_id.title()} not configured for tenant={tenant_id}")
        raise HTTPException(
            status_code=404,
            detail=f"{integration_id.title()} integration not configured",
        )

    if require_api_key and not creds.get("api_key"):
        logger.error(f"{integration_id.title()} api_key missing for tenant={tenant_id}")
        raise HTTPException(
            status_code=404,
            detail=f"{integration_id.title()} credentials incomplete",
        )

    # Build target URL from 'domain' field
    # Domain may include paths, extract just scheme + host
    domain = creds.get("domain", "")
    match = re.match(r"(https?://[^/]+)", domain)
    if match:
        base_url = match.group(1)
    else:
        # Add https if missing
        if not domain.startswith(("http://", "https://")):
            domain = f"https://{domain}"
        base_url = domain.rstrip("/")

    target_url = f"{base_url}/{path}"
    logger.info(f"{integration_id.title()} proxy: forwarding to {target_url}")

    # Build auth headers
    auth_headers = build_auth_headers(integration_id, creds)

    # Forward the request
    forward_headers = {
        "Content-Type": request.headers.get("Content-Type", "application/json"),
        "Accept": request.headers.get("Accept", "application/json"),
        **auth_headers,
    }

    # Get query params
    query_params = dict(request.query_params)

    try:
        async with httpx.AsyncClient(timeout=30.0, verify=ssl_verify) as client:
            body = None
            if request.method in ["POST", "PUT", "PATCH"]:
                body = await request.body()

            response = await client.request(
                method=request.method,
                url=target_url,
                headers=forward_headers,
                params=query_params,
                content=body,
            )

            return Response(
                content=response.content,
                status_code=response.status_code,
                headers={
                    "Content-Type": response.headers.get(
                        "Content-Type", "application/json"
                    )
                },
            )

    except httpx.TimeoutException:
        logger.error(f"{integration_id.title()} request timeout: {target_url}")
        raise HTTPException(
            status_code=504, detail=f"{integration_id.title()} request timed out"
        )
    except httpx.RequestError as e:
        logger.error(f"{integration_id.title()} request error: {e}")
        raise HTTPException(
            status_code=502, detail=f"{integration_id.title()} request failed: {e}"
        )


@app.api_route(
    "/confluence/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def confluence_proxy(path: str, request: Request):
    """Reverse proxy for Confluence API requests.

    Since Confluence URLs are customer-specific (e.g., mycompany.atlassian.net),
    we can't use Envoy's static routing. Instead, this endpoint acts as a reverse
    proxy that:
    1. Validates JWT and extracts tenant context
    2. Looks up the customer's Confluence URL and credentials
    3. Forwards the request with Basic auth to their Confluence instance
    4. Returns the response

    Security: Credentials never leave this service.

    Example:
        Sandbox calls: http://credential-resolver:8002/confluence/wiki/rest/api/content
        This proxies to: https://customer.atlassian.net/wiki/rest/api/content
    """
    import re

    import httpx

    logger.info(f"Confluence proxy: {request.method} /{path}")

    # Validate JWT and extract tenant context
    tenant_id, team_id, sandbox_name = await extract_tenant_context(request)

    logger.info(
        f"Confluence proxy: tenant={tenant_id}, team={team_id}, sandbox={sandbox_name}"
    )

    # Get Confluence credentials
    # Config Service stores: domain (URL), email, api_key
    creds = await get_credentials(tenant_id, team_id, "confluence")
    if not creds or not creds.get("domain") or not creds.get("api_key"):
        logger.error(f"Confluence not configured for tenant={tenant_id}")
        raise HTTPException(
            status_code=404,
            detail="Confluence integration not configured",
        )

    # Build target URL from 'domain' field
    # Domain may include paths like /wiki/home, extract just scheme + host
    domain = creds.get("domain", "")
    match = re.match(r"(https?://[^/]+)", domain)
    if match:
        confluence_url = match.group(1)
    else:
        confluence_url = domain.rstrip("/")

    target_url = f"{confluence_url}/{path}"
    logger.info(f"Confluence proxy: forwarding to {target_url}")

    # Build auth headers
    auth_headers = build_auth_headers("confluence", creds)

    # Forward the request
    # Copy relevant headers from original request
    forward_headers = {
        "Content-Type": request.headers.get("Content-Type", "application/json"),
        "Accept": request.headers.get("Accept", "application/json"),
        **auth_headers,
    }

    # Get query params
    query_params = dict(request.query_params)

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Get request body if present
            body = None
            if request.method in ["POST", "PUT", "PATCH"]:
                body = await request.body()

            response = await client.request(
                method=request.method,
                url=target_url,
                headers=forward_headers,
                params=query_params,
                content=body,
            )

            # Return the response with same status and content
            return Response(
                content=response.content,
                status_code=response.status_code,
                headers={
                    "Content-Type": response.headers.get(
                        "Content-Type", "application/json"
                    )
                },
            )

    except httpx.TimeoutException:
        logger.error(f"Confluence request timeout: {target_url}")
        raise HTTPException(status_code=504, detail="Confluence request timed out")
    except httpx.RequestError as e:
        logger.error(f"Confluence request error: {e}")
        raise HTTPException(status_code=502, detail=f"Confluence request failed: {e}")


# Additional integration proxies for customer-specific URLs
# These use the generic_proxy helper


@app.api_route(
    "/grafana/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def grafana_proxy(path: str, request: Request):
    """Reverse proxy for Grafana API requests."""
    return await generic_proxy("grafana", path, request)


@app.api_route(
    "/elasticsearch/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def elasticsearch_proxy(path: str, request: Request):
    """Reverse proxy for Elasticsearch API requests."""
    # Elasticsearch may have open clusters, so api_key not always required
    # Also disable SSL verify for self-signed certs (common in enterprise)
    return await generic_proxy(
        "elasticsearch", path, request, require_api_key=False, ssl_verify=False
    )


@app.api_route(
    "/prometheus/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def prometheus_proxy(path: str, request: Request):
    """Reverse proxy for Prometheus API requests."""
    # Prometheus auth is optional (many internal deployments are open)
    return await generic_proxy("prometheus", path, request, require_api_key=False)


@app.api_route(
    "/jaeger/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def jaeger_proxy(path: str, request: Request):
    """Reverse proxy for Jaeger API requests."""
    # Jaeger auth is optional (many internal deployments are open)
    return await generic_proxy("jaeger", path, request, require_api_key=False)


@app.api_route(
    "/github/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def github_proxy(path: str, request: Request):
    """Reverse proxy for GitHub API requests (for GitHub Enterprise).

    Note: For github.com, clients can use the fixed URL directly.
    This proxy is for GitHub Enterprise with customer-specific URLs.
    """
    return await generic_proxy("github", path, request)


@app.api_route(
    "/datadog/{path:path}",
    methods=["GET", "POST", "PUT", "DELETE", "PATCH"],
)
async def datadog_proxy(path: str, request: Request):
    """Reverse proxy for Datadog API requests.

    Datadog has site-specific URLs (us1.datadoghq.com, eu1.datadoghq.com, etc.)
    stored in the 'site' field. We use 'domain' generically here.
    """
    import httpx

    logger.info(f"Datadog proxy: {request.method} /{path}")

    # Validate JWT and extract tenant context
    tenant_id, team_id, sandbox_name = await extract_tenant_context(request)

    # Get Datadog credentials (uses 'site' not 'domain')
    creds = await get_credentials(tenant_id, team_id, "datadog")
    if not creds or not creds.get("site") or not creds.get("api_key"):
        logger.error(f"Datadog not configured for tenant={tenant_id}")
        raise HTTPException(
            status_code=404,
            detail="Datadog integration not configured",
        )

    # Build Datadog API URL from site
    site = creds.get("site", "datadoghq.com")
    target_url = f"https://api.{site}/{path}"
    logger.info(f"Datadog proxy: forwarding to {target_url}")

    # Build auth headers
    auth_headers = build_auth_headers("datadog", creds)

    forward_headers = {
        "Content-Type": request.headers.get("Content-Type", "application/json"),
        "Accept": request.headers.get("Accept", "application/json"),
        **auth_headers,
    }

    query_params = dict(request.query_params)

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            body = None
            if request.method in ["POST", "PUT", "PATCH"]:
                body = await request.body()

            response = await client.request(
                method=request.method,
                url=target_url,
                headers=forward_headers,
                params=query_params,
                content=body,
            )

            return Response(
                content=response.content,
                status_code=response.status_code,
                headers={
                    "Content-Type": response.headers.get(
                        "Content-Type", "application/json"
                    )
                },
            )

    except httpx.TimeoutException:
        logger.error(f"Datadog request timeout: {target_url}")
        raise HTTPException(status_code=504, detail="Datadog request timed out")
    except httpx.RequestError as e:
        logger.error(f"Datadog request error: {e}")
        raise HTTPException(status_code=502, detail=f"Datadog request failed: {e}")


@app.api_route(
    "/check", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "HEAD", "OPTIONS"]
)
@app.api_route(
    "/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "HEAD", "OPTIONS"]
)
async def ext_authz_check(request: Request, path: str = ""):
    """Handle ext_authz check from Envoy.

    Envoy sends the original request method and path to the auth service.
    We accept any path and check authorization based on x-original-host header.

    Security: Tenant/team context is extracted from the validated JWT,
    not from headers (which could be spoofed by malicious code in sandbox).
    """
    logger.info(f"ext_authz check: {request.method} {request.url.path}")

    # 1. Validate JWT and extract tenant context
    tenant_id, team_id, sandbox_name = await extract_tenant_context(request)

    # 2. Determine integration from target host and path
    target_host = request.headers.get("x-original-host", "")
    request_path = request.url.path
    logger.info(f"Target host: {target_host}, path: {request_path}")
    integration_id = get_integration_for_host(target_host, request_path)
    logger.info(f"Integration ID mapped: {integration_id}")

    if not integration_id:
        # Passthrough - no credential injection needed
        logger.warning(f"No integration mapping for host: {target_host}")
        return Response(status_code=200)

    logger.info(
        f"Credential request: tenant={tenant_id}, team={team_id}, "
        f"sandbox={sandbox_name}, integration={integration_id}, host={target_host}"
    )

    # 3. Get credentials and validate based on integration type
    creds = await get_credentials(tenant_id, team_id, integration_id)

    # Check if credentials are configured (uses shared logic)
    if not is_integration_configured(integration_id, creds):
        logger.error(
            f"No credentials found for {integration_id} "
            f"(tenant={tenant_id}, team={team_id})"
        )
        raise HTTPException(
            status_code=500,
            detail=f"Credentials not configured for {integration_id}",
        )

    # 4. Build auth headers and return them as HTTP response headers
    # Envoy's ext_authz will forward these based on allowed_upstream_headers config
    headers_to_add = build_auth_headers(integration_id, creds)
    logger.info(
        f"Injecting headers for {integration_id}: {list(headers_to_add.keys())}"
    )

    return Response(status_code=200, headers=headers_to_add)


async def extract_tenant_context(request: Request) -> tuple[str, str, str]:
    """Extract tenant/team context from JWT (secure) or headers (permissive mode).

    Security: In strict mode (production), we ONLY trust the JWT.
    In permissive mode (local dev), we fall back to headers if JWT is missing.

    Returns:
        Tuple of (tenant_id, team_id, sandbox_name)
    """
    jwt_token = request.headers.get("x-sandbox-jwt", "")

    # Try to validate JWT
    claims = validate_sandbox_jwt(jwt_token)

    if claims:
        logger.debug(f"JWT validated for sandbox: {claims.sandbox_name}")
        return claims.tenant_id, claims.team_id, claims.sandbox_name

    # JWT validation failed
    if JWT_MODE == "strict":
        logger.error("JWT validation failed in strict mode - rejecting request")
        raise HTTPException(
            status_code=401,
            detail="Invalid or missing sandbox JWT",
        )

    # Permissive mode: fall back to headers (for local dev only)
    logger.warning("JWT validation failed - falling back to headers (permissive mode)")
    tenant_id = request.headers.get("x-tenant-id", "local")
    team_id = request.headers.get("x-team-id", "local")
    return tenant_id, team_id, "unknown"


async def get_credentials(
    tenant_id: str, team_id: str, integration_id: str
) -> dict | None:
    """Get credentials from configured source."""
    if CREDENTIAL_SOURCE == "environment":
        # Local/self-hosted: load from env vars
        return ENV_CREDENTIALS.get(integration_id)

    # SaaS: fetch from Config Service (cached)
    cache_key = (tenant_id, team_id, integration_id)
    if cache_key in credential_cache:
        return credential_cache[cache_key]

    if config_client is None:
        logger.error("Config Service client not initialized")
        return None

    creds = await config_client.get_integration_config(
        tenant_id, team_id, integration_id
    )
    if creds:
        credential_cache[cache_key] = creds

    return creds


def build_auth_headers(integration_id: str, creds: dict) -> dict[str, str]:
    """Build authentication headers for the integration.

    For customers using our shared Anthropic key, adds attribution metadata for cost tracking.
    This applies to ALL customers using our key (trial users AND paid users who don't BYOK).

    Note: Some integrations (like Confluence) use direct credential fetch instead of
    proxy injection because their client libraries manage auth internally.
    """
    import base64

    if integration_id == "anthropic":
        api_key = creds.get("api_key", "")
        headers = {"x-api-key": api_key}

        # Add attribution for ALL customers using our shared key (for cost tracking/billing)
        # This includes: trial users + paid users who choose not to bring their own key
        workspace = creds.get("workspace_attribution")
        if workspace:
            # Use custom header for internal attribution tracking
            # Note: Anthropic forwards custom headers in their logs for cost analysis
            headers["x-incidentfox-workspace"] = workspace
            headers["x-incidentfox-tenant"] = workspace  # Redundant but explicit
            logger.info(f"Added cost attribution for workspace: {workspace}")

        return headers

    elif integration_id == "coralogix":
        # Coralogix uses Bearer token
        api_key = creds.get("api_key", "")
        return {"Authorization": f"Bearer {api_key}"}

    elif integration_id == "confluence":
        # Confluence uses Basic auth (email:api_key base64 encoded)
        email = creds.get("email", "")
        api_key = creds.get("api_key", "")
        if email and api_key:
            auth_string = f"{email}:{api_key}"
            encoded = base64.b64encode(auth_string.encode()).decode()
            return {"Authorization": f"Basic {encoded}"}
        logger.warning("Confluence credentials incomplete for Basic auth")
        return {}

    elif integration_id == "datadog":
        # Datadog uses two API keys in headers
        return {
            "DD-API-KEY": creds.get("api_key", ""),
            "DD-APPLICATION-KEY": creds.get("app_key", ""),
        }

    elif integration_id == "elasticsearch":
        # Elasticsearch uses Basic auth (username:password) or API key
        username = creds.get("username", "")
        api_key = creds.get("api_key", "")
        if username and api_key:
            # Basic auth mode
            auth_string = f"{username}:{api_key}"
            encoded = base64.b64encode(auth_string.encode()).decode()
            return {"Authorization": f"Basic {encoded}"}
        elif api_key:
            # API key mode (already base64 encoded id:key format)
            return {"Authorization": f"ApiKey {api_key}"}
        # No auth (open cluster)
        return {}

    elif integration_id == "github":
        # GitHub uses Bearer token
        api_key = creds.get("api_key", "")
        return {"Authorization": f"Bearer {api_key}"}

    elif integration_id == "incident_io":
        # incident.io uses Bearer token
        api_key = creds.get("api_key", "")
        return {"Authorization": f"Bearer {api_key}"}

    elif integration_id in ["grafana", "prometheus", "kubernetes"]:
        # These use Bearer token
        api_key = creds.get("api_key", "")
        if api_key:
            return {"Authorization": f"Bearer {api_key}"}
        return {}

    elif integration_id == "jaeger":
        # Jaeger typically doesn't need auth, but support Bearer if provided
        api_key = creds.get("api_key", "")
        if api_key:
            return {"Authorization": f"Bearer {api_key}"}
        return {}

    # Default: Bearer token
    api_key = creds.get("api_key", "")
    return {"Authorization": f"Bearer {api_key}"}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8002)
