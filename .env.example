# ============================================
# Slack Configuration
# ============================================

# Bot Token from OAuth & Permissions
SLACK_BOT_TOKEN=xoxb-your-bot-token

# App Token for Socket Mode (from Basic Information -> App-Level Tokens)
SLACK_APP_TOKEN=xapp-your-app-token

# ============================================
# AI Configuration (required)
# ============================================

# Anthropic API Key (required — Claude is the default model)
ANTHROPIC_API_KEY=sk-ant-your-api-key

# ============================================
# Alternative AI Models (optional)
# ============================================
# Set LLM_MODEL to use a non-Claude model. Format: provider/model-name
# You can use ANY model the provider offers. The proxy translates all
# requests from Claude SDK format automatically.
#
# See docs/DEPLOYMENT.md for the full guide with all providers and examples.
#
# Quick examples:
#   LLM_MODEL=openai/gpt-4o-mini           # or gpt-4o, o1, o3-mini, etc.
#   LLM_MODEL=gemini/gemini-2.5-flash      # or gemini-2.5-pro, etc.
#   LLM_MODEL=deepseek/deepseek-chat       # or deepseek-reasoner
#   LLM_MODEL=mistral/mistral-small-latest # or mistral-large-latest, codestral-latest
#   LLM_MODEL=xai/grok-3-mini             # or grok-3
#   LLM_MODEL=moonshot/kimi-k2-turbo-preview
#   LLM_MODEL=minimax/MiniMax-Text-01
#
# Cloud platforms (access many models through one account):
#   LLM_MODEL=bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0
#   LLM_MODEL=bedrock/meta.llama3-1-70b-instruct-v1:0
#   LLM_MODEL=azure_ai/your-deployment-name
#   LLM_MODEL=openrouter/meta-llama/llama-3.1-70b-instruct
#   LLM_MODEL=openrouter/anthropic/claude-3.5-haiku
#
# Then set the API key for that provider below.
#LLM_MODEL=

# --- Direct provider API keys (set the one matching your LLM_MODEL) ---
#OPENAI_API_KEY=sk-your-openai-key
#GEMINI_API_KEY=your-gemini-key
#DEEPSEEK_API_KEY=your-deepseek-key
#MISTRAL_API_KEY=your-mistral-key
#XAI_API_KEY=your-xai-key
#MOONSHOT_API_KEY=your-moonshot-key
#MINIMAX_API_KEY=your-minimax-key

# --- OpenRouter (300+ models with one API key — openrouter.ai/models) ---
#OPENROUTER_API_KEY=sk-or-your-openrouter-key

# --- AWS Bedrock (Claude, Llama, Mistral, etc. via AWS) ---
# Option A: Bearer token (from Bedrock console -> API keys)
#AWS_BEARER_TOKEN_BEDROCK=your-bedrock-bearer-token
# Option B: IAM credentials
#AWS_ACCESS_KEY_ID=your-access-key
#AWS_SECRET_ACCESS_KEY=your-secret-key
#AWS_REGION=us-east-1

# --- Azure AI Foundry (serverless deployments — ai.azure.com) ---
# Model name after azure_ai/ must match your deployment name in Azure portal
#AZURE_AI_API_KEY=your-azure-ai-key
#AZURE_AI_API_BASE=https://your-endpoint.services.ai.azure.com

# ============================================
# Optional: Observability & Tracing
# ============================================

# Laminar tracing (for debugging agent behavior)
#LMNR_PROJECT_API_KEY=lm_your-key

# Coralogix integration (for log/metric queries)
#CORALOGIX_API_KEY=your-coralogix-key
#CORALOGIX_DOMAIN=https://yourteam.app.cx498.coralogix.com

# ============================================
# Optional: AWS Integration
# ============================================

# AWS credentials (for CloudWatch, EKS, EC2 queries — separate from Bedrock)
#AWS_ACCESS_KEY_ID=your-access-key
#AWS_SECRET_ACCESS_KEY=your-secret-key
#AWS_REGION=us-west-2

# ============================================
# Advanced Configuration
# ============================================

# SRE Agent URL (default: http://sre-agent:8000)
# Only change if running sre-agent separately
#SRE_AGENT_URL=http://sre-agent:8000
